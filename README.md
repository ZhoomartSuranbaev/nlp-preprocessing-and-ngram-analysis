# ğŸ“˜ **NLP Text Cleaning and N-grams**

This project demonstrates a complete NLP preprocessing pipeline applied to raw English text collected from Wikipedia. It covers text extraction, cleaning, tokenization, lemmatization, stopword removal, and N-gram analysis (unigrams, bigrams, trigrams).
All experiments were performed in Google Colab.

---

## ğŸš€ **Project Goals**

* Collect raw English text from Wikipedia
* Clean and normalize the text
* Perform tokenization and lemmatization
* Remove English stopwords
* Generate N-grams (1â€“3)
* Visualize the most frequent N-grams
* Build a simple and reproducible NLP preprocessing workflow

---

## ğŸ“‚ **Project Structure**

```
project/
â”‚
â”œâ”€â”€ NLP_Text_Cleaning.ipynb        # Main Google Colab notebook
â”œâ”€â”€ README.md                      # Project documentation
â””â”€â”€ requirements.txt (optional)    # Python dependencies
```

---

## ğŸ“Š **Dataset**

The raw dataset is collected directly from Wikipedia using the `wikipedia` Python package.

**Wikipedia pages used:**

* *Artificial intelligence*
* *Machine learning*
* *Data science*

The text is split into sentences and stored in a pandas DataFrame.

---

## ğŸ§¹ **Text Cleaning Steps**

Each text entry goes through the following preprocessing pipeline:

1. Lowercasing
2. Removing punctuation
3. Removing numbers
4. Tokenization
5. Removing stopwords
6. Lemmatization
7. Returning clean tokens

Implemented using:

* `nltk.word_tokenize`
* `nltk.corpus.stopwords`
* `WordNetLemmatizer`

---

## ğŸ”  **N-gram Generation**

Custom function `get_ngrams(tokens_list, n)` generates:

* **Unigrams (n=1)**
* **Bigrams (n=2)**
* **Trigrams (n=3)**

Frequencies are computed using:

```python
from collections import Counter
```

---

## ğŸ“ˆ **Visualization**

The top 10 most frequent:

* Unigrams
* Bigrams
* Trigrams

are visualized using **matplotlib** bar charts.

---

## ğŸ›  **Dependencies**

Install all required packages:

```bash
pip install pandas numpy nltk matplotlib wikipedia
```

Download NLTK data inside the notebook:

```python
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')
```

---

## â–¶ï¸ **How to Run**

Open the notebook in Google Colab:

1. Clone the repository
2. Open `.ipynb` notebook
3. Run all cells
4. Review the output DataFrames and visualizations

---

## ğŸ“Œ **Result**

The project produces:

âœ” A cleaned dataset with tokenized and lemmatized text
âœ” Unigram, bigram, and trigram frequency lists
âœ” Visual charts of top N-grams
âœ” A reusable NLP preprocessing pipeline

---

## ğŸ“œ **License**

This project is open-source and available under the MIT License.

---

## ğŸ¤ **Contributions**

Pull requests and improvements are welcome.
Feel free to fork the project and experiment with other datasets or NLP methods.
